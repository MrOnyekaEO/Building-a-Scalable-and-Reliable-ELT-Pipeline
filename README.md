# Building-a-Scalable-and-Reliable-ELT-Pipeline

---
## Functional ELT pipeline using dbt, Snowflake, and Apache Airflow.
---

![ELT pipeline](https://github.com/user-attachments/assets/3f84ee4e-aab9-4793-bcd5-5ef51def7eef)

---
## Project Overview
### **Objective**: To create a robust and scalable data pipeline using dbt, Snowflake, and Apache Airflow to extract, load, and transform (ELT) data for analysis and reporting.
---
### **Tools and Technologies:**

**dbt**: A data transformation tool for building and managing data pipelines.

**Snowflake**: A cloud-based data warehouse.

**Apache Airflow**: A workflow orchestration platform.

---
**Project Structure**

**Data Source**: Identify the source of your data (e.g., CSV files, APIs, databases).
**Data Extraction**: Use dbt to extract data from the source and load it into Snowflake.
**Data Transformation**: Apply transformations using dbt models to clean, normalize, and prepare the data for analysis.
**Data Loading**: Load the transformed data into Snowflake for storage and querying.
**Orchestration**: Use Apache Airflow to schedule and manage the entire pipeline.

Step-by-Step Guide
1. Set up the Environment:

2. Create dbt Models:

3. Build the Airflow DAG:

4. Run the Pipeline:

Code: https://bittersweet-mall-f00.notion.site/Code-along-build-an-ELT-Pipeline-in-1-Hour-dbt-Snowflake-Airflow-cffab118a21b40b8acd3d595a4db7c15
